import streamlit as st
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import plotly.express as px

st.set_page_config(page_title="Mg&DS Dashboard", layout="wide")
st.title("üß† Website Activity Analysis")

# Load the datasets (replace with your raw GitHub URLs)
actions_url = "https://raw.githubusercontent.com/vitormiguel99/onest4/refs/heads/main/TheDashboard/data/cleaned_action.csv"
clicks_url = "https://raw.githubusercontent.com/vitormiguel99/onest4/refs/heads/main/TheDashboard/data/cleaned_click_session.csv"

actions_df = pd.read_csv(actions_url)
click_sessions_df = pd.read_csv(clicks_url)

# Cast yyyymmdd columns as timestamps (handle invalid entries gracefully)
def safe_to_datetime(series):
    return pd.to_datetime(series.astype(str), format='%Y%m%d', errors='coerce')

actions_df['action_yyyymmdd'] = safe_to_datetime(actions_df['action_yyyymmdd'])
click_sessions_df['click_yyyymmdd'] = safe_to_datetime(click_sessions_df['click_yyyymmdd'])
click_sessions_df['session_yyyymmdd'] = safe_to_datetime(click_sessions_df['session_yyyymmdd'])

# Tabs
tabs = st.tabs(["üè† Home", "üìà Overview", "ü´±üèª‚Äçü´≤üèºEngagement", "üìä Classification", "üß† Clustering"])

# üè† Home
with tabs[0]:
    st.header("Welcome to the Mg&DS Analysis Dashboard")
    st.markdown("""
        This interactive dashboard helps you explore data regarding the activity in the Website Management & Data Science.
        The analyses are separated between:
        üìà Overview -> That gives you a global view about the activity, prioritizing the visualisation of KPIs and distributions
        üìä Classification -> 
        üß† Clustering ->

        Use the tabs above to navigate through the analysis.
    """)

# üìà Overview
with tabs[1]:
    st.header("üìä Usage & Interaction Analysis")

    # Q1: Distribution Over Time
    st.subheader("1. Distribution of Actions, Clicks, and Sessions Over Time")
    actions_by_date = actions_df.groupby('action_yyyymmdd').size().reset_index(name='nb_actions')
    clicks_by_date = click_sessions_df.groupby('click_yyyymmdd').size().reset_index(name='nb_clicks')
    sessions_by_date = click_sessions_df.drop_duplicates('click_session_id').groupby('session_yyyymmdd').size().reset_index(name='nb_sessions')

    fig_actions = px.line(actions_by_date, x='action_yyyymmdd', y='nb_actions', title="Actions Over Time", markers=True)
    fig_clicks = px.line(clicks_by_date, x='click_yyyymmdd', y='nb_clicks', title="Clicks Over Time", markers=True)
    fig_sessions = px.line(sessions_by_date, x='session_yyyymmdd', y='nb_sessions', title="Sessions Over Time", markers=True)

    st.plotly_chart(fig_actions, use_container_width=True)
    st.plotly_chart(fig_clicks, use_container_width=True)
    st.plotly_chart(fig_sessions, use_container_width=True)

    # Q2: Top 5 Users
    st.subheader("2. Top 5 Users by Actions, Clicks, and Sessions")
    top_actions = actions_df['action_visitor_id'].value_counts().head(5).reset_index(name='nb_actions')
    top_clicks = click_sessions_df['click_visitor_id'].value_counts().head(5).reset_index(name='nb_clicks')
    top_sessions = click_sessions_df.drop_duplicates('click_session_id')['click_visitor_id'].value_counts().head(5).reset_index(name='nb_sessions')

    col1, col2, col3 = st.columns(3)
    with col1:
        st.write("üîù Actions")
        st.dataframe(top_actions)
    with col2:
        st.write("üîù Clicks")
        st.dataframe(top_clicks)
    with col3:
        st.write("üîù Sessions")
        st.dataframe(top_sessions)

    # Q3: Avg Interactions per User
    st.subheader("3. Average Number of Interactions per User")
    avg_actions_per_user = actions_df.groupby('action_visitor_id').size().mean()
    avg_clicks_per_user = click_sessions_df.groupby('click_visitor_id').size().mean()
    avg_sessions_per_user = click_sessions_df.drop_duplicates('click_session_id').groupby('click_visitor_id').size().mean()

    st.metric("Avg Actions/User", f"{avg_actions_per_user:.2f}")
    st.metric("Avg Clicks/User", f"{avg_clicks_per_user:.2f}")
    st.metric("Avg Sessions/User", f"{avg_sessions_per_user:.2f}")

    # Q4: Clicks per Session
    st.subheader("4. Clicks per Session")
    clicks_per_session = click_sessions_df.groupby('click_session_id').size()
    st.write(f"**Avg**: {clicks_per_session.mean():.2f} | **Max**: {clicks_per_session.max()} | **Min**: {clicks_per_session.min()}")

    # Q5: Actions per Session
    st.subheader("5. Actions per Session (from Actions File Only)")
    actions_per_session = actions_df.groupby('action_session_id').size()
    st.write(f"**Avg**: {actions_per_session.mean():.2f} | **Max**: {actions_per_session.max()} | **Min**: {actions_per_session.min()}")

    # Q6: Actions per Type
    if "action_name" in actions_df.columns:
        st.subheader("6. Actions per Type")
        type_counts = actions_df.groupby('action_name').size().reset_index(name='count')
        fig_type = px.bar(type_counts, x='action_name', y='count', title="Actions per Type")
        st.plotly_chart(fig_type, use_container_width=True)

        # Q7: Actions per Type per Session
        st.subheader("7. Actions per Type per Session")
        type_session = actions_df.groupby(['action_session_id', 'action_name']).size().reset_index(name='count')
        stats = type_session.groupby('action_name')['count'].agg(['mean', 'max', 'min']).reset_index()
        st.dataframe(stats)
    else:
        st.warning("‚ö†Ô∏è 'action_name' column not found in actions dataset.")

# ü´±üèª‚Äçü´≤üèºEngagement
with tabs[2]:
    st.header("ü´±üèª‚Äçü´≤üèºEngagement Analysis")

    #Q1 Single metrics 
    # üî¢ Taux de rebond
    st.subheader("2. Bounce Rate and Return Rate")
    total_sessions = click_sessions_df["click_session_id"].nunique()
    total_bounces = click_sessions_df[click_sessions_df["session_is_bounce"] == 1]["click_session_id"].nunique()
    taux_de_rebond = (total_bounces / total_sessions) * 100
    # üîÅ Taux de retour
    nb_total_utilisateurs = click_sessions_df["click_visitor_id"].nunique()
    sessions_par_utilisateur = click_sessions_df.groupby("click_visitor_id")["click_session_id"].nunique().reset_index()
    utilisateurs_revenus = sessions_par_utilisateur[sessions_par_utilisateur["click_session_id"] > 1].shape[0]
    taux_de_retour = (utilisateurs_revenus / nb_total_utilisateurs) * 100

    col4, col5 = st.columns(2)
    with col4:
        st.metric("Taux de rebond", f"{taux_de_rebond:.2f} %")
    with col5:
        st.metric("Taux de retour", f"{taux_de_retour:.2f} %")
    # üìÑ Moyenne de pages vues par session
    st.subheader("3. Moyenne de pages vues par session")
    pages_par_session = click_sessions_df.groupby("click_session_id")["click_num_pageviews"].max().reset_index(name="session_num_pageviews")
    moyenne_pages = pages_par_session["session_num_pageviews"].mean()
    st.metric("Pages vues/session", f"{moyenne_pages:.2f}")
    
    # Q2. SCORE D'ENGAGEMENT = Prend en compte les action(calss√©es par importance) et le parma√©tre de r√©gularit√© de l'utilisateur
    st.subheader("2. Engagement Score: Takes into account the user's actions (ranked by importance) and regularity parameter")
    
    # üßΩ Pr√©paration
    actions_df["action_timestamp"] = pd.to_datetime(actions_df["action_timestamp"], unit="s", errors="coerce")

    # Pond√©ration des groupes d‚Äôactions
    poids_action_group = {
        'publish': 5,
        'animate': 4,
        'participate': 3,
        'reaction': 2,
        'user': 1
    }
    actions_df["action_group_weight"] = actions_df["action_group"].map(poids_action_group)
    
    # Supprimer les lignes sans poids
    df_clean = actions_df.dropna(subset=["action_group_weight"])
    
    # Calcul du score brut
    user_scores = df_clean.groupby("action_visitor_id").agg(
        score_brut=("action_group_weight", "sum"),
        frequence=("action_group_weight", "count"),
        is_repeat_visitor=("action_is_repeat_visitor", "max")
    ).reset_index()
    
    # Score pond√©r√© par fr√©quence (logarithmique)
    user_scores["score_actions"] = user_scores["score_brut"] * np.log1p(user_scores["frequence"])
    
    # Normalisation des actions
    score_max = user_scores["score_actions"].max()
    user_scores["score_action_normalise"] = user_scores["score_actions"] / score_max
    
    # ‚öñÔ∏è Nouveau calcul du score : 80 % actions, 20 % fid√©lit√©
    user_scores["score_engagement"] = (
        0.8 * user_scores["score_action_normalise"] +
        0.2 * user_scores["is_repeat_visitor"]
    ) * 100
    
    # R√©sultat final
    result = user_scores[["action_visitor_id", "score_engagement"]].sort_values(by="score_engagement", ascending=False).round(2)

    # üîç Search box
    visitor_filter = st.text_input("Search for a specific Visitor ID")
    if visitor_filter:
        filtered_result = result[result["action_visitor_id"].astype(str).str.contains(visitor_filter)]
        st.dataframe(filtered_result)
    else:
        st.dataframe(result.head(10))

# üìä Classification
with tabs[3]:
    st.header("üìä Classification")
    st.info("This section will display classification models and performance metrics.")

# üß† Clustering
with tabs[4]:
    st.header("üß† Clustering")
    st.info("This section will display final results, interpretations, and export options.")
